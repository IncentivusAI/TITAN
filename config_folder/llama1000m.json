{
    "architectures": [
        "LLaMAForCausalLM"
    ],
    "bos_token_id": 0,
    "eos_token_id": 1,
    "activation_function": "silu",
    "embedding_dim": 640,
    "ffn_hidden_dim": 1708,
    "init_stddev": 0.02,
    "context_length": 1024,
    "model_family": "llama",
    "attention_heads": 10,
    "layer_count": 12,
    "padding_token_id": -1,
    "norm_epsilon": 1e-06,
    "transformer_lib_version": "4.28.1",
    "enable_cache": true,
    "token_count": 32100,
    "organization": "ICT"
}
